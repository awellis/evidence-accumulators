{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('../src')))\n",
    "from constant_drift_diffusion import *\n",
    "from accumulators import *\n",
    "from motion_simulation import *\n",
    "\n",
    "# bayesflow\n",
    "sys.path.append(os.path.abspath(os.path.join('../../BayesFlow')))\n",
    "from bayesflow.networks import InvariantNetwork, InvertibleNetwork\n",
    "from bayesflow.amortizers import SingleModelAmortizer\n",
    "from bayesflow.trainers import ParameterEstimationTrainer\n",
    "from bayesflow.diagnostics import *\n",
    "\n",
    "from tensorflow.keras.layers import Dense, GRU, LSTM, Conv1D, MultiHeadAttention, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation \n",
    "N_SIM = 500\n",
    "N_OBS = 100\n",
    "\n",
    "# bayesflow\n",
    "PARAM_NAMES = [\"a\", \"ndt\", \"bias\", \"kappa\"]\n",
    "N_PARAMS = len(PARAM_NAMES)\n",
    "N_EPOCHS = 50\n",
    "ITER_PER_EPOCH = 1000\n",
    "BATCH_SIZE = 32\n",
    "N_SAMPLES = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulator Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 100\n",
    "a     = 3.0\n",
    "ndt   = 0.2\n",
    "bias  = 0.5\n",
    "kappa = 5\n",
    "theta = np.array([a, ndt, bias, kappa])\n",
    "\n",
    "unique_motions = np.array([-0.725, -0.675, -0.625, -0.575, -0.525, 0.525,  0.575,  0.625,  0.675,  0.725], dtype=np.float32)\n",
    "amplitude = np.repeat(unique_motions, 10)\n",
    "condition = to_categorical(pd.factorize(amplitude)[0])\n",
    "\n",
    "rt, resp = const_dm_simulator(theta, n_obs, amplitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack((np.expand_dims(rt, axis=1), np.expand_dims(resp, axis=1), np.expand_dims(amplitude, axis=1))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_, x_ = const_dm_batch_simulator(32, 100)\n",
    "x_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSummary(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, meta_inv, n_out=10):\n",
    "        super(CustomSummary, self).__init__()\n",
    "        self.inv = InvariantNetwork(meta_inv)\n",
    "        self.out = Dense(n_out)\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.out(self.inv(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_meta = {\n",
    "    'n_dense_s1': 2,\n",
    "    'n_dense_s2': 2,\n",
    "    'n_dense_s3': 2,\n",
    "    'n_equiv':    2,\n",
    "    'dense_s1_args': {'activation': 'relu', 'units': 32},\n",
    "    'dense_s2_args': {'activation': 'relu', 'units': 32},\n",
    "    'dense_s3_args': {'activation': 'relu', 'units': 32},\n",
    "}\n",
    "\n",
    "# invertable inference network\n",
    "inf_meta = {\n",
    "    'n_coupling_layers': 4,\n",
    "    's_args': {\n",
    "        'units': [128, 128],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    't_args': {\n",
    "        'units': [128, 128],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    'alpha': 1.9,\n",
    "    'permute': True,\n",
    "    'use_act_norm': True,\n",
    "    'n_params': N_PARAMS\n",
    "}\n",
    "\n",
    "inference_net = InvertibleNetwork(inf_meta)\n",
    "summary_net = CustomSummary(sum_meta)\n",
    "amortizer = SingleModelAmortizer(inference_net, summary_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning-rate decay\n",
    "learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    0.0005, 1000, 0.99, staircase=True\n",
    ")\n",
    "\n",
    "trainer = ParameterEstimationTrainer(\n",
    "    network=amortizer, \n",
    "    generative_model=const_dm_batch_simulator,\n",
    "    learning_rate=learning_rate,\n",
    "    checkpoint_path='../src/selected_checkpoints/const_dm2',\n",
    "    clip_value=3,\n",
    "    max_to_keep=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning-rate decay\n",
    "# trainer.optimizer = tf.keras.optimizers.Adam(0.00007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# losses = trainer.train_online(35, ITER_PER_EPOCH, BATCH_SIZE, n_obs=N_OBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramter Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate and amortized inference\n",
    "p_, x_ = const_dm_batch_simulator(n_sim=N_SIM,n_obs=N_OBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = amortizer.sample(x_, n_samples=N_SAMPLES)\n",
    "param_means = samples.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovery plot\n",
    "f = true_vs_estimated(theta_true=p_, theta_est=param_means,\n",
    "                  param_names=PARAM_NAMES, dpi=300, figsize=(24,6),font_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate\n",
    "n_sbc = 5000\n",
    "n_post_samples_sbc = 250\n",
    "params, sim_data = const_dm_batch_simulator(n_sbc, N_OBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amortized inference\n",
    "param_samples = np.concatenate([amortizer.sample(x, n_post_samples_sbc)\n",
    "                                for x in tf.split(sim_data, 10, axis=0)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank-plot\n",
    "f = plot_sbc(param_samples, params, param_names=PARAM_NAMES, figsize=(24, 8), bins=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Eye Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation\n",
    "true_params, sim_data = const_dm_batch_simulator(N_SIM, N_OBS)\n",
    "\n",
    "# Amortized inference\n",
    "param_samples = np.concatenate([amortizer.sample(x, N_SAMPLES)\n",
    "                                for x in tf.split(sim_data, 10, axis=0)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Posterior z-score\n",
    "# Compute posterior means and stds\n",
    "post_means = param_samples.mean(1)\n",
    "post_stds = param_samples.std(1)\n",
    "post_vars = param_samples.var(1)\n",
    "\n",
    "# Compute posterior z score\n",
    "post_z_score = (post_means - true_params) / post_stds\n",
    "\n",
    "### Posterior contraction, i.e., 1 - post_var / prior_var\n",
    "prior_a = (0.5, 0.1, 0.2, 0.0) # lower bound of uniform prior\n",
    "prior_b = (3.0, 0.5, 0.8, 5.0) # upper bound of uniform prior\n",
    "\n",
    "# Compute prior vars analytically\n",
    "prior_vars = np.array([(b-a)**2/12 for a,b in zip(prior_a, prior_b)])\n",
    "post_cont = 1 - post_vars / prior_vars\n",
    "\n",
    "# Plotting time\n",
    "f, axarr = plt.subplots(1, 4, figsize=(24, 6))\n",
    "for i, (p, ax) in enumerate(zip(PARAM_NAMES, axarr.flat)):\n",
    "\n",
    "\n",
    "    ax.scatter(post_cont[:, i], post_z_score[:, i], color='#8f2727', alpha=0.7)\n",
    "    ax.set_title(p, fontsize=20)\n",
    "    sns.despine(ax=ax)\n",
    "    ax.set_xlim([-0.1, 1.05])\n",
    "    ax.set_ylim([-3.5, 3.5])\n",
    "    ax.grid(color='black', alpha=0.1)\n",
    "    ax.set_xlabel('Posterior contraction', fontsize=14)\n",
    "    if i == 0 or i == 3:\n",
    "        ax.set_ylabel('Posterior z-score', fontsize=14)\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Retrodictive Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "directory = str(Path().absolute())\n",
    "path = str(Path(directory).parents[0]) + '/data/single_sub_data.csv'\n",
    "data = np.loadtxt(open(path, 'rb'), delimiter=\",\", skiprows=1)\n",
    "\n",
    "# subset data\n",
    "data_subset = data[(data[:, 1] == 1) & (data[:, 2] == 1)]\n",
    "\n",
    "# get one hot encoded amplitude\n",
    "amplitude = data_subset[:, 4]\n",
    "condition = get_hot_encoded_amplitude(amplitude)\n",
    "\n",
    "# prepare data for amortized inference\n",
    "final_data = np.hstack((np.expand_dims(data_subset[:, 6], axis=1),\n",
    "                        np.expand_dims(data_subset[:, 5], axis=1), condition))\n",
    "\n",
    "final_data = np.expand_dims(final_data, axis=0)\n",
    "\n",
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = amortizer.sample(final_data, n_samples=N_SAMPLES)\n",
    "sns.pairplot(pd.DataFrame(samples, columns=PARAM_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sim = samples.shape[0]\n",
    "n_obs = final_data.shape[1]\n",
    "amplitude = np.round(data_subset[:, 4], 3)\n",
    "condition = final_data[:, 2:]\n",
    "pred_data = const_dm_simulator_pp_check(samples, amplitude, condition, n_sim, n_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = np.quantile(pred_data, [0.025, 0.5, 0.975], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_amplitude = np.sort(np.unique(amplitude))\n",
    "emp_resp_prop = np.empty(10)\n",
    "\n",
    "\n",
    "for i in range(len(unique_amplitude)):\n",
    "    tmp_data = data_subset[(np.round(data_subset[:, 4], 3) == unique_amplitude[i]), 5]\n",
    "    emp_resp_prop[i] = tmp_data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_amplitude = np.sort(np.unique(amplitude))\n",
    "plt.plot(range(len(unique_amplitude)), quantiles[1], linestyle='dashed')\n",
    "plt.xticks(range(len(unique_amplitude)), unique_amplitude, rotation=45)\n",
    "plt.fill_between(range(len(unique_amplitude)), quantiles[0], quantiles[2],\n",
    "                    alpha=0.2, label=\"Predictive Uncertainty\")\n",
    "plt.ylim([0.0, 1.0])\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = data_subset[(np.round(data_subset[:, 4], 3) == unique_amplitude[0]), 5]\n",
    "tmp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a30ec19090e2f2b1a106537d5c312817a43a20ac3d56ad8096fbad2e54ec1c01"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
